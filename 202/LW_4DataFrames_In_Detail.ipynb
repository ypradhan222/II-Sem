{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b59c22",
   "metadata": {
    "id": "b6b59c22"
   },
   "source": [
    "Hi there,\n",
    "One of our brother suggested it is good to have an interactive lesson so here is an attampt for this.\n",
    "\n",
    "In this notebook, we will explain **Data frames** in detail.\n",
    "\n",
    "So each exercises will have few bullet points, with the topics and a sample code.\n",
    "There will be some questions at the end of the notebook which you are supposed to answer them and submit as Lab Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add53003",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "add53003",
    "outputId": "5ab77907-bc36-4e67-d871-30ce80fc4d6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 11:05:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/04 11:05:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').appName('Om_Sairam').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471296f",
   "metadata": {
    "id": "4471296f"
   },
   "source": [
    "# DataFrame\n",
    "- It is basically \"A distributed collection of data grouped into named columns\"\n",
    "- Unlike datasets, dataframes are loosely typed.\n",
    "- One can also create PySpark DataFrame from different data sources like TXT, CSV, JSON, ORV, Avro, Parquet, XML formats by reading from HDFS, and clod platforms\n",
    "- There are multiple ways to create dataframe, the most generic one is using `spark.read`\n",
    "- In our previous assignment, we used `iris_dataset = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"irisdata.csv\")`\n",
    "    - We are telling spark to take the file, infer its schema and also the provided csv has header.\n",
    "- Now we will try to create schema manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22f466",
   "metadata": {
    "id": "8e22f466"
   },
   "source": [
    "### Schema\n",
    "- A schema defines the column names and types of a DataFrame\n",
    "- A schema is a `StructType` made up of a number of fields, `StructFields`, that have a name, type, a Boolean flag specifying whether that column can contain missing or null values\n",
    "- One can even insert random metadata in the schema as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c23594",
   "metadata": {
    "id": "16c23594"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Iris DataFrame Headers: p_w;p_l;s_w;s_l;type\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m myManualSchema \u001b[38;5;241m=\u001b[39m \u001b[43mStructType\u001b[49m([\n\u001b[1;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_w\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_l\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms_w\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      6\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms_l\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      7\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Now we defined schema, now lets create the data frame and use the above schema\u001b[39;00m\n\u001b[1;32m     10\u001b[0m iris_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miris_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, schema\u001b[38;5;241m=\u001b[39mschema, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
     ]
    }
   ],
   "source": [
    "#Iris DataFrame Headers: p_w;p_l;s_w;s_l;type\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"p_w\", FloatType(), True),\n",
    "    StructField(\"p_l\", FloatType(), True),\n",
    "    StructField(\"s_w\", FloatType(), True),\n",
    "    StructField(\"s_l\", FloatType(), True),\n",
    "    StructField(\"type\", StringType(), True, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "#Now we defined schema, now lets create the data frame and use the above schema\n",
    "iris_df = spark.read.csv(\"iris_dataset.csv\", schema=schema, sep=\",\")\n",
    "#One can even create dataframe from rdd using createDataFrame method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82d7e8",
   "metadata": {
    "id": "2e82d7e8"
   },
   "source": [
    "## Columns and Expressions\n",
    "- Columns in Spark are similar to columns in a spreadsheet\n",
    "- It cannot be used outside the context of the DataFrame\n",
    "    - To have a real value in column, we should have `row` which will be inside of `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13ad91",
   "metadata": {
    "id": "9d13ad91"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n",
    "#Different ways of creating columns\n",
    "#If you want to use specific column in a dataframe, df\n",
    "iris_df.col(\"p_w\")#Just eg.\n",
    "iris_df.columns#Displays all the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d14bfd",
   "metadata": {
    "id": "c5d14bfd"
   },
   "source": [
    "### Expressions\n",
    "- An expression is a set of transformations on one or more values in a record in a DataFrame\n",
    "- The `expr()` function is used to express transformations or computations involving DataFrame columns.\n",
    "- If it is bit confusing just remember the following:\n",
    "- **Columns are just expressions.**\n",
    "- **Columns and transformations of those columns compile to the same logical plan as parsed expressions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1122afc",
   "metadata": {
    "id": "c1122afc"
   },
   "source": [
    "## Record and Rows\n",
    "- In Spark, each row in a DataFrame is a single record. Spark represents this record as an object of type `Row`\n",
    "- Spark manipulates Row objects using column expressions in order to produce usable values\n",
    "- Row objects internally represent arrays of bytes.\n",
    "- There is abstraction present here, making us to use the column expression to manipulate them.\n",
    "- Itâ€™s important to note that only DataFrames have schemas. Rows themselves do not have schemas.\n",
    "- When creating Row manually, one must specify the values in the same order as the schema of the DataFrame to which they might be appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49f583",
   "metadata": {
    "id": "1b49f583"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819f9ef",
   "metadata": {
    "id": "e819f9ef"
   },
   "outputs": [],
   "source": [
    "# So let us stich it altogether\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"some\", StringType(), True),\n",
    "    StructField(\"col\", StringType(), True),\n",
    "    StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()\n",
    "#Before Running the code, try guessing the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec17d7",
   "metadata": {
    "id": "d0ec17d7"
   },
   "source": [
    "## Note Worthy Points\n",
    "- By default Spark is case insensitive; one can make Spark case sensitive by setting the configuration:\n",
    "    - `set spark.sql.caseSensitive true`\n",
    "- Sometimes we need to cast the spark columns to different datatypes. It can be done:\n",
    "    - `df.withColumn(\"count2\", col(\"count\").cast(\"long\"))`\n",
    "    - `withColumn` is used to create new columns.\n",
    "- To rename a column, we will use `df.withColumnRenamed(\"OLD_NAME\", \"new_name\")`\n",
    "- Take a guess on how do we drop the columns.\n",
    "- *Remeber to reduce the partition size from 200 to 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2bf08e",
   "metadata": {
    "id": "ab2bf08e"
   },
   "outputs": [],
   "source": [
    "## Selecting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064db92",
   "metadata": {
    "id": "f064db92"
   },
   "source": [
    "## Filtering\n",
    "- To filter rows, we create an expression that evaluates to true or false.\n",
    "- The rows, to which the expression is evaluated as false, are *filtered out*\n",
    "- There are two methods to perform this operation: `where` or `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c75dcd",
   "metadata": {
    "id": "05c75dcd"
   },
   "outputs": [],
   "source": [
    "df.filter(col(\"count\") < 2).show(2)\n",
    "df.where(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d86a15",
   "metadata": {
    "id": "41d86a15"
   },
   "source": [
    "## Unique\n",
    "- A very common use case is to extract the unique or distinct values in a DataFrame\n",
    "- We use `distinct` function for the following.\n",
    "- It is a transformation, so it will return a new data frame with only unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9723a",
   "metadata": {
    "id": "61f9723a"
   },
   "outputs": [],
   "source": [
    "df.select(\"col1\", \"col2\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21f47c",
   "metadata": {
    "id": "ae21f47c"
   },
   "source": [
    "## Random\n",
    "- Sometimes, you  just want to sample some random records from your DataFrame.\n",
    "- It can be perofmred using `sample` method on a DataFrame\n",
    "- It is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048124e",
   "metadata": {
    "id": "1048124e"
   },
   "outputs": [],
   "source": [
    "seed = 5#Seed should be provided for better random behaviour.\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12900163",
   "metadata": {
    "id": "12900163"
   },
   "outputs": [],
   "source": [
    "## Sorting\n",
    "- To sort a df based on the column, one can use `sort` and `orderBy`\n",
    "- To more explicitly specify sort direction,use the `asc` and `desc` functions if operating bon a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df6328",
   "metadata": {
    "id": "f8df6328"
   },
   "outputs": [],
   "source": [
    "df.sort(\"col\").show(5)\n",
    "df.orderBy(\"col2\", \"col\").show(5)\n",
    "df.orderBy(col(\"col1\"), col(\"col2\")).show(5)#Also  FIne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0246f",
   "metadata": {
    "id": "f5d0246f"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"col desc\")).show(2)#Note the usage of expr\n",
    "df.orderBy(col(\"col\").desc(), col(\"col2\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2960b3",
   "metadata": {
    "id": "db2960b3"
   },
   "outputs": [],
   "source": [
    "#Lab Work\n",
    "- Use the MTCars data set to answer the folling questions.\n",
    "1. Create the dataframe by specifying the Manual Schema\n",
    "2. Rename all the columns to something for your liking\n",
    "3. Show the distinct cars based on the number of cylinders\n",
    "4. Sort the dataframe based on the milage of the car.\n",
    "5. Your friend is planning to buy a new car in a pocket friendly manner. So allocate a score to all cars in your data frame\n",
    "    Eg: - Create a column called `score`.\n",
    "        - Come up with a formula that provides score, say :\n",
    "                - milage is important so 0.2 * value of milage + 0.5 * # of cyl ... so on\n",
    "6. Just for Fun add a new Row into the Data frame for Nano\n",
    " Details: Nano;Manual;25kmpl;2Cyl;\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
